{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane Finding with OpenCV\n",
    "\n",
    "Lane finding is an important initial step to any autonomous driving pipeline. Once the lanes are identified they can be used to keep a vehicle from drifting into other lanes. This requires our lane finding algorithm to be robust to varying light conditions and weather conditions. This repo will be a basic tutorial on how to implement a robust lane finding algorithm using OpenCV. The steps of this algortihm are:\n",
    "\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image to identify lane pixels.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "The goal of the algortihm is to take raw images and be able to transform them into images with lanes draw on them as show below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<figure>\n",
    " <img src=\"./test_images/straight_lines1.jpg\" width=\"300\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Input Image </p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    "<figure>\n",
    " <img src=\"./output_images/straight_lines1.png\" width=\"400\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Output Image</p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "\n",
    "img = mpimg.imread('./camera_cal/calibration5.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Calibration\n",
    "Cameras that use lenses typically distort images by stretching them unevenly. This is due to the light rays being refracted differently depending on where they enter the lense. Fortunately OpenCV allows us to calibrate our camera by taking pictures of chessboards at different angles and mapping the detected corners to where the corners would appear in an undistorted image. The chessboard images can be found in '/camera_cal/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\\n\\nax1.imshow(img)\\nax1.set_title('Original Image', fontsize=40)\\n\\nax2.imshow(undistored_img)\\nax2.set_title('Undistorted Image', fontsize=40)\\nplt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calibrate_camera():\n",
    "\n",
    "    cal_images = glob.glob('camera_cal/calibration*.jpg')\n",
    "    nx, ny = 9, 6\n",
    "\n",
    "    objpoints = []  # 3D points\n",
    "    imgpoints = []  # 2D points\n",
    "\n",
    "    objp = np.zeros((nx*ny,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nx,0:ny].T.reshape(-1, 2)\n",
    "\n",
    "    fname = cal_images[0]\n",
    "    for fname in cal_images:\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],None,None)\n",
    "\n",
    "    return mtx, dist\n",
    "\n",
    "cam_mtx, cam_dist = calibrate_camera()\n",
    "undistored_img = cv2.undistort(img, cam_mtx, cam_dist, None, cam_mtx)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Original Image', fontsize=40)\n",
    "\n",
    "ax2.imshow(undistored_img)\n",
    "ax2.set_title('Undistorted Image', fontsize=40)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"./output_images/calibration.png\" width=\"990\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Output Image</p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Transforms and Gradients\n",
    "Now that we have an undistorted image we can start identifying which pixels in the image we may think are lane pixels. The most common approach is to run a filter through the image and then threshold the output. Here I have chosen to use a laplacian filter since it tracks the delta in both dimensions but it is by no means a requirement. \n",
    "\n",
    "After closer inspection one will find that for certain lighting and road conditions applying a filter on a graysclae image doesn't work as wells as we would like. We can instead transform our image to the HLS color space and then apply our laplacian filter to the saturation and luminosity channels. Additionally we can also create color masks by thresholding the raw pixel values. We combine the output from our filtered image along with yellow and white color masks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24, 9))\\n\\nax1.imshow(img)\\nax1.set_title('Original Image', fontsize=40)\\nax2.imshow(yellow_mask+white_mask, cmap='gray')\\nax2.set_title('Color Masks', fontsize=40)\\nax3.imshow(s_lap_binary+l_lap_binary, cmap='gray')\\nax3.set_title('Laplacian Output', fontsize=40)\\nax4.imshow(combined, cmap='gray')\\nax4.set_title('Binary Image', fontsize=40)\\nplt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = mpimg.imread('./test_images/test4.jpg')\n",
    "img = cv2.undistort(img, cam_mtx, cam_dist, None, cam_mtx)\n",
    "\n",
    "def laplacian_threshold(img, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "\n",
    "    lapacian = cv2.Laplacian(img, cv2.CV_64F,ksize=sobel_kernel)\n",
    "    abs_lapacian = np.absolute(lapacian)\n",
    "    scaled_lapacian = np.uint8(255*(abs_lapacian/np.max(abs_lapacian)))\n",
    "\n",
    "    binary_output =  np.zeros_like(abs_lapacian)\n",
    "    binary_output[(abs_lapacian >= mag_thresh[0]) & (abs_lapacian <= mag_thresh[1])] = 1\n",
    "\n",
    "    return binary_output\n",
    "\n",
    "def color_mask(hsv,img, min_thresh, max_thresh):\n",
    "\n",
    "    mask = cv2.inRange(hsv, min_thresh, max_thresh)\n",
    "    res = cv2.bitwise_and(img,img, mask= mask)\n",
    "    return mask, res\n",
    "\n",
    "HLS = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2HLS)\n",
    "L = HLS[:,:,1]\n",
    "S = HLS[:,:,2]\n",
    "k_size = 3\n",
    "s_lap_binary = laplacian_threshold(S, sobel_kernel=k_size, mag_thresh=(75, 255))\n",
    "l_lap_binary = laplacian_threshold(L, sobel_kernel=k_size, mag_thresh=(75, 255))\n",
    "\n",
    "HSV = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2HSV)\n",
    "yellow_mask, yellow_img = color_mask(HSV,img, np.array([0,100,100]), np.array([60,255,255]))\n",
    "white_mask, white_img = color_mask(HSV,img, np.array([20, 0, 180]), np.array([255,80,255]))\n",
    "\n",
    "combined = np.zeros_like(s_lap_binary)\n",
    "combined[(s_lap_binary == 1) | (l_lap_binary == 1)] = 1\n",
    "combined[(yellow_mask > 0)|(white_mask > 0)] = 1\n",
    "\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24, 9))\n",
    "\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Original Image', fontsize=40)\n",
    "ax2.imshow(yellow_mask+white_mask, cmap='gray')\n",
    "ax2.set_title('Color Masks', fontsize=40)\n",
    "ax3.imshow(s_lap_binary+l_lap_binary, cmap='gray')\n",
    "ax3.set_title('Laplacian Output', fontsize=40)\n",
    "ax4.imshow(combined, cmap='gray')\n",
    "ax4.set_title('Binary Image', fontsize=40)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"./output_images/binary.png\" width=\"990\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Output Image</p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspective Transform and Fitting Lanes\n",
    "With our binary image we are now in good shape to identify our lane boundaries, the tricky part however is how to handle lanes that curve. To get a better understanding of the curvature of our lanes it would be helpful to be able to look at the lane from a birds eye perspective. OpenCV allows us to change perspective by identifying source points and choosing how we would like to see those points positioned in the new perspective. In other words we can choose our source points to be points on the lane that currently are in a trapezoidal shape and choose the destination points to be a rectangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\\nax1.imshow(combined, cmap='gray')\\nax1.set_title('Normal View', fontsize=40)\\nax2.imshow(birds_eye, cmap='gray')\\nax2.set_title('Birds Eye', fontsize=40)\\nplt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perspective_change(img,inv=False):\n",
    "\n",
    "    h, w = img.shape[0:2]\n",
    "    src = np.array([[570. /1280*w, 465./720*h],\n",
    "                    [715. /1280*w, 465./720*h],\n",
    "                    [1100./1280*w, 720./720*h],\n",
    "                    [200. /1280*w, 720./720*h]], np.float32)\n",
    "\n",
    "    dst = np.array([[300. /1280*w, 0./720*h],\n",
    "                    [1000./1280*w, 0./720*h],\n",
    "                    [1000./1280*w, 720./720*h],\n",
    "                    [300. /1280*w, 720./720*h]], np.float32)\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "\n",
    "    if inv:\n",
    "        warped_img = cv2.warpPerspective(img, Minv, (w,h))\n",
    "    else:\n",
    "        warped_img = cv2.warpPerspective(img, M, (w,h))\n",
    "\n",
    "    return warped_img\n",
    "\n",
    "birds_eye = perspective_change(combined, inv=False)\n",
    "birds_eye[birds_eye > 0 ] = 1\n",
    "birds_eye = birds_eye.astype('uint8')\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "ax1.imshow(combined, cmap='gray')\n",
    "ax1.set_title('Normal View', fontsize=40)\n",
    "ax2.imshow(birds_eye, cmap='gray')\n",
    "ax2.set_title('Birds Eye', fontsize=40)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"./output_images/birdseye.png\" width=\"990\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Output Image</p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must find the pixels corresponding to each lane. To do this we can look at a histogram of the x values of each nonzero pixel. We can find where the pixels are concentrated and use the peaks as the base of our lanes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"./output_images/histogram.png\" width=\"300\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Histogram</p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the base we separate the image into slices. Starting from the bottom for each slice we find all the pixels that are in a certain window of the base of the lane. We then use those pixels to recenter the window and find all the lane pixels for the next slice. We repeat this process as we move up the lane. Finally we fit a polynomial to all the pixels that we have gathered for each lane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nout_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\\nout_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\\nplt.imshow(out_img)\\nplt.plot(left_fitx, ploty, color='yellow')\\nplt.plot(right_fitx, ploty, color='yellow')\\nplt.xlim(0, 1280)\\nplt.ylim(720, 0)\\n\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwindows = 9\n",
    "window_height = np.int(birds_eye.shape[0]/nwindows)\n",
    "out_img = np.dstack((birds_eye,birds_eye,birds_eye))*255\n",
    "margin = 50\n",
    "minpix = 50\n",
    "\n",
    "nonzero = birds_eye.nonzero()\n",
    "nonzeroy = np.array(nonzero[0])\n",
    "nonzerox = np.array(nonzero[1])\n",
    "\n",
    "\n",
    "histogram = np.sum(birds_eye[birds_eye.shape[0]//2:,:], axis=0)\n",
    "midpoint = np.int(histogram.shape[0]/2)\n",
    "leftx_current  = np.argmax(histogram[:midpoint])\n",
    "rightx_current = np.argmax(histogram[midpoint:])+midpoint\n",
    "\n",
    "left_lane_inds = []\n",
    "right_lane_inds = []\n",
    "\n",
    "for window in range(nwindows):\n",
    "\n",
    "    win_y_low = birds_eye.shape[0] - (window+1)*window_height\n",
    "    win_y_high = birds_eye.shape[0] - window*window_height\n",
    "    win_xleft_low = leftx_current - margin\n",
    "    win_xleft_high = leftx_current + margin\n",
    "    win_xright_low = rightx_current - margin\n",
    "    win_xright_high = rightx_current + margin\n",
    "\n",
    "    cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2)\n",
    "    cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2)\n",
    "    # Identify the nonzero pixels in x and y within the window\n",
    "    good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "    good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "\n",
    "    # Append these indices to the lists\n",
    "    left_lane_inds.append(good_left_inds)\n",
    "    right_lane_inds.append(good_right_inds)\n",
    "\n",
    "    # If you found > minpix pixels, recenter next window on their mean position\n",
    "    if len(good_left_inds) > minpix:\n",
    "        leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "    if len(good_right_inds) > minpix:\n",
    "        rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "# Concatenate the arrays of indices\n",
    "left_lane_inds = np.concatenate(left_lane_inds)\n",
    "right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "leftx = nonzerox[left_lane_inds]\n",
    "lefty = nonzeroy[left_lane_inds] \n",
    "rightx = nonzerox[right_lane_inds]\n",
    "righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "# Fit a second order polynomial to each\n",
    "left_fit = np.polyfit(lefty, leftx, 2)\n",
    "right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "ploty = np.linspace(0, birds_eye.shape[0]-1, birds_eye.shape[0] )\n",
    "left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "plt.imshow(out_img)\n",
    "plt.plot(left_fitx, ploty, color='yellow')\n",
    "plt.plot(right_fitx, ploty, color='yellow')\n",
    "plt.xlim(0, 1280)\n",
    "plt.ylim(720, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"./output_images/polyfit.png\" width=\"400\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Output Image</p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radius of Curvature\n",
    "Next we would like to know the curvature of the lane which can be calculated directly from the coefficients of our fitted polynomial. We also scale our (x,y) coordinates from pixels to meters to get a better real world understanding of the curvature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "xm_per_pix = 3.7/700 # meteres per pixel in x dimension\n",
    "\n",
    "def rad_curv(xarray, yarray):\n",
    "    fit = np.polyfit(yarray*ym_per_pix, xarray*xm_per_pix, 2)\n",
    "    y_eval = np.max(yarray*ym_per_pix)\n",
    "    curverad = (1 + (2*fit[0]*y_eval + fit[1])**2)**1.5/2/fit[0]\n",
    "    return curverad\n",
    "\n",
    "y_eval = np.max(lefty)\n",
    "left_lane_radius_of_curvature = rad_curv(leftx, lefty)\n",
    "right_lane_radius_of_curvature = rad_curv(rightx, righty)\n",
    "lane_pos = (left_fitx[-1] - right_fitx[-1])//2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally with both lanes fitted and our curvature calculated we have all the information we need to project the lane onto our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfont = cv2.FONT_HERSHEY_SIMPLEX\\n\\ncv2.putText(final,'Left radius of curvature = %.2f m'%(left_lane_radius_of_curvature),(50,50), font, 1,(255,255,255),2,cv2.LINE_AA)\\ncv2.putText(final,'Right radius of curvature = %.2f m'%(right_lane_radius_of_curvature),(50,80), font, 1,(255,255,255),2,cv2.LINE_AA)\\ncv2.putText(final,'Vehicle position offset = %.2f m'%((lane_pos - (final.shape[1]//2))*xm_per_pix),(50,110), font, 1,(255,255,255),2,cv2.LINE_AA)\\n\\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\\nax1.imshow(img)\\nax1.set_title('Input Image', fontsize=40)\\nax2.imshow(final)\\nax2.set_title('Final Output', fontsize=40)\\nplt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_margin = 10\n",
    "left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-draw_margin, ploty]))])\n",
    "left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+draw_margin, ploty])))])\n",
    "left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-draw_margin, ploty]))])\n",
    "right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+draw_margin, ploty])))])\n",
    "right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "between_line_pts = np.hstack((left_line_window2, right_line_window1))\n",
    "\n",
    "window_img = np.zeros_like(np.dstack((birds_eye,birds_eye,birds_eye)))\n",
    "cv2.fillPoly(window_img, np.int_([left_line_pts]), (255,255,255))\n",
    "cv2.fillPoly(window_img, np.int_([right_line_pts]), (255,255,255))\n",
    "cv2.fillPoly(window_img, np.int_([between_line_pts]), (0,100, 255))\n",
    "normal_view = perspective_change(window_img, inv=True)\n",
    "final = cv2.addWeighted(img, 1, normal_view, 0.75, 0)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "cv2.putText(final,'Left radius of curvature = %.2f m'%(left_lane_radius_of_curvature),(50,50), font, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "cv2.putText(final,'Right radius of curvature = %.2f m'%(right_lane_radius_of_curvature),(50,80), font, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "cv2.putText(final,'Vehicle position offset = %.2f m'%((lane_pos - (final.shape[1]//2))*xm_per_pix),(50,110), font, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Input Image', fontsize=40)\n",
    "ax2.imshow(final)\n",
    "ax2.set_title('Final Output', fontsize=40)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"./output_images/final.png\" width=\"990\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Output Image</p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "This pipline works pretty well on a few test images but when used on a video stream there were some problems. The pipline was still able to detect the lanes in the right ball park but the detected lanes were very jitttery. To alleviate this problem I used pixels gathered from previous **n** frames as well as the current frame to smooth out the lanes transistions. Another technique that also helped was to remove outlier pixels that were far away from most of pixels in the lane.\n",
    "\n",
    "Other interesting techniques that I did not have time to try were:\n",
    "* Doing a weighted average of the previous pixels\n",
    "* Rejecting frames that resulted in a significant change in the curvature\n",
    "* Doing a weighted average of the polynomial coefficients\n",
    "\n",
    "The current pipline has vunerablities in its reliance on the color masks since the thresholds required will change for different settings. The  The techniques above should definitely be tried in order to avoid the current pitfalls of the system.\n",
    "\n",
    "Here is link to the output from the pipline on a video stream:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/T5kUmBSBjLI/0.jpg)](https://www.youtube.com/watch?v=T5kUmBSBjLI)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
